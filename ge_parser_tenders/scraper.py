from __future__ import annotations
from concurrent.futures import ThreadPoolExecutor, as_completed


"""
scraper.py ‚Äï –≥—Ä—É–∑–∏–Ω—Å–∫–∏–µ —Ç–µ–Ω–¥–µ—Ä—ã
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* –ø—Ä–æ—Ö–æ–¥–∏—Ç –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º —Å–∞–π—Ç–∞, –∫–ª–∏–∫–∞–µ—Ç ¬´·Éì·Éù·Éô·É£·Éõ·Éî·Éú·É¢·Éê·É™·Éò·Éê¬ª
* —Å–∫–∞—á–∏–≤–∞–µ—Ç –≤—Å–µ –ø—Ä–∏–∫—Ä–µ–ø–ª—ë–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
* –¥–∞—ë—Ç –∏–º —á–∏—Ç–∞–µ–º—ã–µ ASCII‚Äë–∏–º–µ–Ω–∞ (slug‚Äëified), —á—Ç–æ–±—ã –≤–Ω–µ—à–Ω–∏–µ CLI‚Äë—É—Ç–∏–ª–∏—Ç—ã
  –≤—Ä–æ–¥–µ *pdftotext* –Ω–µ —Å–ø–æ—Ç—ã–∫–∞–ª–∏—Å—å –æ —é–Ω–∏–∫–æ–¥‚Äë–ø—É—Ç–∏
* –∏—â–µ—Ç –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç ID —Ç–µ–Ω–¥–µ—Ä–∞,
  –µ—Å–ª–∏ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —Ñ–∞–π–ª ¬´—Å—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç¬ª

–í–ï–†–°–ò–Ø 2025‚Äë05‚Äë18
----------------
‚Ä¢ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∞ –Ω–∞–≤–∏–≥–∞—Ü–∏—è –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º: –∫–Ω–æ–ø–∫–∞ ¬´Next¬ª –∏–Ω–æ–≥–¥–∞ –Ω–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–ª–∞,
  –∏–∑‚Äë–∑–∞ —á–µ–≥–æ –ø–∞—Ä—Å–µ—Ä –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–ª —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É. –¢–µ–ø–µ—Ä—å –º—ã –∂–¥—ë–º
  –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã —á–µ—Ä–µ–∑ EC.staleness_of –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∫–Ω–æ–ø–∫–∞ –Ω–µ
  –æ—Ç–∫–ª—é—á–µ–Ω–∞.
"""

import random
import logging
import mimetypes
import re
import shutil
import tempfile
import time
import socket
from pathlib import Path
from typing import List, Set
from urllib.parse import unquote, urlparse

import requests
from selenium.common.exceptions import (
    ElementNotInteractableException,
    StaleElementReferenceException,
    TimeoutException,
)
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.remote.webelement import WebElement
from slugify import slugify
from tqdm import tqdm


from .config import ParserSettings
from .memory_manager import MemoryManager
from .driver_utils import make_driver, wait_click
from .extractor import file_contains_keywords


# ---------------------------------------------------------------------------
#                               helpers
# ---------------------------------------------------------------------------
socket.setdefaulttimeout(60)
_CD_FILENAME_RX = re.compile(
    r"""filename\*?          # filename –∏–ª–∏ filename*
        (?:=[^']*'')?        # =utf-8''  (–º–æ–∂–µ—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å)
        ["']?                # –≤–æ–∑–º–æ–∂–Ω–∞—è –∫–∞–≤—ã—á–∫–∞
        (?P<name>[^";]+)     # —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ –∏–º—è
    """,
    re.I | re.X,
)


def _decode_maybe_utf8(s: str) -> str:
    """–ü–æ–ø—ã—Ç–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫, –ø—Ä–∏—à–µ–¥—à–∏–π –±–∏—Ç—ã–º UTF‚Äë8."""
    try:
        return s.encode("latin1").decode("utf-8")
    except UnicodeDecodeError:
        return s


def _filename_from_cd(cd: str | None) -> str | None:
    if not cd:
        return None

    # RFC¬†5987 ‚Äî filename*=utf-8''%E1%83%93%E1%83%90‚Ä¶
    if "filename*" in cd:
        _, value = cd.split("filename*", 1)[1].split("=", 1)
        if "''" in value:
            enc, _, raw = value.partition("''")
            return unquote(raw, encoding=enc, errors="replace")

    # filename="·Éì·Éê·Éú·Éê·É†·Éó·Éò.pdf"
    m = _CD_FILENAME_RX.search(cd)
    if m:
        name = m["name"].strip().strip('"')
        return _decode_maybe_utf8(name)

    return None


_CT_EXT_MAP = {
    "application/pdf": ".pdf",
    "application/msword": ".doc",
    "application/vnd.openxmlformats-officedocument.wordprocessingml.document": ".docx",
    "application/vnd.ms-excel": ".xls",
    "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet": ".xlsx",
    "text/html": ".html",
    "application/zip": ".zip",
}


def _ext_from_content_type(ct: str | None) -> str:
    if not ct:
        return ""
    ct = ct.split(";", 1)[0].strip()
    return mimetypes.guess_extension(ct) or _CT_EXT_MAP.get(ct, "")


# ‚îÄ‚îÄ –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ ASCII‚Äë–∏–º—è + —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def _safe_filename(raw: str, ext_hint: str = "") -> str:
    stem, ext = Path(raw).stem, Path(raw).suffix or ext_hint
    slug = slugify(stem, lowercase=False, separator="_")
    if not slug:
        slug = "file"
    return f"{slug}{ext}"


def _unique(path: Path) -> Path:
    if not path.exists():
        return path
    stem, ext = path.stem, path.suffix
    idx = 1
    while True:
        candidate = path.with_name(f"{stem}_{idx}{ext}")
        if not candidate.exists():
            return candidate
        idx += 1


# ‚îÄ‚îÄ –ø–∞–≥–∏–Ω–∞—Ü–∏—è ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def _has_next_page(driver) -> bool:
    """Return True if the ¬´Next¬ª button is enabled."""
    try:
        next_btn = driver.find_element(By.ID, "btn_next")
        disabled = (
            not next_btn.is_enabled()
            or "ui-state-disabled" in next_btn.get_attribute("class" or "")
            or next_btn.get_attribute("disabled")
        )
        return not disabled
    except Exception:
        return False


def _next_page(driver, timeout: int = 30):
    """Click ¬´Next¬ª and wait for the table to refresh."""
    if not _has_next_page(driver):
        raise StopIteration("no next page")

    pager = driver.find_element(By.CSS_SELECTOR, ".pager")
    driver.execute_script("arguments[0].scrollIntoView({block:'center'});", pager)
    next_btn = driver.find_element(By.ID, "btn_next")

    # save reference to the first row ‚Äî we'll wait until it's stale
    first_row = driver.find_element(By.CSS_SELECTOR, "#list_apps_by_subject tbody tr")

    driver.execute_script("arguments[0].click();", next_btn)

    # wait until previous rows become stale ‚Üí table was rebuilt
    WebDriverWait(driver, timeout).until(EC.staleness_of(first_row))
    # ‚Ä¶and new rows appear
    WebDriverWait(driver, timeout).until(
        EC.presence_of_element_located((By.CSS_SELECTOR, "#list_apps_by_subject tbody tr"))
    )


# ‚îÄ‚îÄ –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –∫–ª–∏–∫ –ø–æ —Å—Ç—Ä–æ–∫–µ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def safe_click(driver, element, retries: int = 3):
    for _ in range(retries):
        try:
            driver.execute_script("arguments[0].scrollIntoView({block:'center'});", element)
            WebDriverWait(driver, 10).until(lambda d: element.is_displayed() and element.is_enabled())
            element.click()
            return
        except (StaleElementReferenceException, ElementNotInteractableException):
            time.sleep(0.5)
    # —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ ‚Äì JavaScript‚Äë–∫–ª–∏–∫
    driver.execute_script("arguments[0].click();", element)


# ---------------------------------------------------------------------------
#                               main scraper
# ---------------------------------------------------------------------------

# --- –ø–µ—Ä–µ–¥ scrape_tenders() –¥–æ–±–∞–≤—å—Ç–µ —Ö–µ–ª–ø–µ—Ä --------------------------
def _download_and_check(url: str,
                        display_name: str,
                        session: requests.Session,
                        downloads_dir: Path,
                        settings: ParserSettings,
                        memory_manager: MemoryManager | None) -> bool:
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç —Ñ–∞–π–ª, —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–∞–ø–∫—É –∏ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True, –µ—Å–ª–∏ —Å—Ä–∞–±–æ—Ç–∞–ª —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω keyword.
    """
    # c–∫–∞—á–∏–≤–∞–µ–º (3 —Ä–µ—Ç—Ä–∞—è –∫–∞–∫ –ø—Ä–µ–∂–¥–µ)
    for attempt in range(3):
        try:
            resp = session.get(url, stream=True, timeout=30)
            resp.raise_for_status()
            break
        except (requests.exceptions.ConnectionError,
                requests.exceptions.ChunkedEncodingError,
                requests.exceptions.Timeout) as exc:
            if attempt == 2:
                logging.warning("–ù–µ —Å–∫–∞—á–∞–Ω %s (%s)", url, exc)
                return False
            sleep_time = 2 ** attempt + (0.1 * attempt) # —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
            time.sleep(sleep_time)
        except Exception as exc:
            logging.error("–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ %s: %s", url, exc)
            return False

    cd_name = _filename_from_cd(resp.headers.get("Content-Disposition"))
    name = cd_name or display_name or Path(url).name
    if "." not in Path(name).name:
        name += _ext_from_content_type(resp.headers.get("Content-Type"))

    out_path = _unique(downloads_dir / _safe_filename(name))
    with out_path.open("wb") as f:
        for chunk in resp.iter_content(8192):
            f.write(chunk)

    # –ø—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
    try:
        return file_contains_keywords(out_path, settings=settings, memory_manager=None)
    finally:
        # –Ω–µ –¥–µ—Ä–∂–∏–º –º–µ—Å—Ç–æ –Ω–∞ –¥–∏—Å–∫–µ ‚Äì —É–¥–∞–ª—è–µ–º —Å—Ä–∞–∑—É
        out_path.unlink(missing_ok=True)



def scrape_tenders(max_pages: int | None = None, *, headless: bool = True, settings: ParserSettings,) -> List[str]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ ID —Ç–µ–Ω–¥–µ—Ä–æ–≤, –≤ —á—å–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –Ω–∞–π–¥–µ–Ω—ã –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞."""
    memory_manager = MemoryManager(
        warning_threshold_mb=settings.memory_warning_threshold_mb,
        critical_threshold_mb=settings.memory_critical_threshold_mb,
        gc_interval=settings.gc_interval_seconds
    )
    hits: List[str] = []
    logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")

    DOWNLOADS_DIR = Path("downloads")
    if DOWNLOADS_DIR.exists():
        shutil.rmtree(DOWNLOADS_DIR)
    DOWNLOADS_DIR.mkdir(parents=True, exist_ok=True)

    cache_path = Path("visited_ids.txt")
    visited: Set[str] = set(cache_path.read_text().split()) if cache_path.exists() else set()

    with tempfile.TemporaryDirectory() as tmpdir:
        try:
            driver = make_driver(headless=headless, download_dir=Path(tmpdir))
            driver.get(str(settings.start_url))               # ‚Üê cast to str

            root = "{uri.scheme}://{uri.netloc}".format(uri=urlparse(str(settings.start_url)))

            # —Å–∫–æ–ø–∏—Ä—É–µ–º cookie –≤ requests.Session ‚Üí —ç–∫–æ–Ω–æ–º–∏–º –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—é
            session = requests.Session()
            session.headers.update({
                "User-Agent": "Mozilla/5.0 (compatible; scraper/1.0)",
                "Connection": "keep-alive",
                "Accept-Encoding": "gzip, deflate",
            })
            for c in driver.get_cookies():
                session.cookies.set(c["name"], c["value"])

            # —Ñ–∏–ª—å—Ç—Ä ¬´·Éí·Éê·Éõ·Éê·É†·ÉØ·Éï·Éî·Éë·É£·Éö·Éò ·Éí·Éê·Éõ·Éù·Éï·Éö·Éî·Éú·Éò·Éö·Éò·Éê¬ª (–ø–æ–±–µ–¥–∏—Ç–µ–ª—å –æ–ø—Ä–µ–¥–µ–ª—ë–Ω)
            wait_click(driver, (By.ID, "app_donor_id"))
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.XPATH, "//option[contains(., '·Éí·Éê·Éõ·Éê·É†·ÉØ·Éï·Éî·Éë·É£·Éö·Éò ·Éí·Éê·Éõ·Éù·Éï·Éö·Éî·Éú·Éò·Éö·Éò·Éê')]"))
            )
            wait_click(driver, (By.XPATH, "//option[contains(., '·Éí·Éê·Éõ·Éê·É†·ÉØ·Éï·Éî·Éë·É£·Éö·Éò ·Éí·Éê·Éõ·Éù·Éï·Éö·Éî·Éú·Éò·Éö·Éò·Éê')]"))
            wait_click(driver, (By.ID, "search_btn"))
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "#list_apps_by_subject tbody tr"))
            )

            page = 1
            while True:
                logging.info("Page %d", page)
                rows = driver.find_elements(By.CSS_SELECTOR, "#list_apps_by_subject tbody tr")

                for idx in tqdm(range(len(rows)), desc=f"Page {page}", unit="tender"):
                    # rows = driver.find_elements(By.CSS_SELECTOR, "#list_apps_by_subject tbody tr")
                    if idx >= len(rows):
                        break

                    tender_row = rows[idx]
                    tender_id = tender_row.find_element(By.CSS_SELECTOR, "p strong").text.strip()
                    if tender_id in visited:
                        continue
                    visited.add(tender_id)

                    safe_click(driver, tender_row)
                    # ‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –Ω–∞ –≤–∫–ª–∞–¥–∫–µ ¬´·É®·Éî·Éó·Éê·Éï·Éê·Éñ·Éî·Éë·Éî·Éë·Éò¬ª ‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï
                    try:
                        wait_click(driver, (By.XPATH, "//a[contains(., '·É®·Éî·Éó·Éê·Éï·Éê·Éñ·Éî·Éë·Éî·Éë·Éò')]"))
                        WebDriverWait(driver, 10).until(
                            EC.presence_of_all_elements_located(
                                (By.CSS_SELECTOR, "#app_bids table.ktable tbody tr")
                            )
                        )
                        cand_cells: list[WebElement] = driver.find_elements(
                            By.CSS_SELECTOR,
                            "#app_bids table.ktable tbody tr td:nth-child(1)",
                        )
                        candidates: list[str] = [c.text.strip() for c in cand_cells if c.text.strip()]
                        firm_found = any(settings.excluded_firm.lower() in c.lower() for c in candidates)
                        logging.info(
                            "   –ù–∞–π–¥–µ–Ω–Ω—ã–µ –∫–∞–Ω–¥–∏–¥–∞—Ç—ã: %s. –ö–∞–Ω–¥–∏–¥–∞—Ç–∞ ·É®·Éû·É° ,,·Éò·Éú·Éí·Éò-77 %s",
                            ", ".join(candidates) or "‚Äî",
                            "–Ω–∞–π–¥–µ–Ω" if firm_found else "–Ω–µ –Ω–∞–π–¥–µ–Ω–æ",
                        )
                        if firm_found:
                            # –Ω–∞–∑–∞–¥ –∏ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–µ–Ω–¥–µ—Ä
                            wait_click(driver, (By.ID, "back_button_2"))
                            WebDriverWait(driver, 30).until(
                                EC.presence_of_element_located(
                                    (By.CSS_SELECTOR, "#list_apps_by_subject tbody tr")
                                )
                            )
                            continue
                    except Exception as exc:
                        logging.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ (%s) ‚Äì –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º", exc)

                    # ¬´·Éì·Éù·Éô·É£·Éõ·Éî·Éú·É¢·Éê·É™·Éò·Éê¬ª
                    WebDriverWait(driver, 30).until(
                        EC.element_to_be_clickable((By.XPATH, "//a[contains(., '·Éì·Éù·Éô·É£·Éõ·Éî·Éú·É¢·Éê·É™·Éò·Éê')]"))
                    )
                    wait_click(driver, (By.XPATH, "//a[contains(., '·Éì·Éù·Éô·É£·Éõ·Éî·Éú·É¢·Éê·É™·Éò·Éê')]"))
                    try:
                        WebDriverWait(driver, 10).until(
                            EC.presence_of_all_elements_located((By.CSS_SELECTOR, "div.answ-file a"))
                        )
                        links = driver.find_elements(By.CSS_SELECTOR, "div.answ-file a")
                        logging.info("  –ù–∞–π–¥–µ–Ω–æ %d –≤–ª–æ–∂–µ–Ω–∏–π test", len(links))
                    
                    except TimeoutException:
                        try:
                            WebDriverWait(driver, 10).until(
                                EC.presence_of_all_elements_located((By.CSS_SELECTOR, "#tender_docs td.obsolete0 a"))
                            )
                            links = driver.find_elements(By.CSS_SELECTOR, "#tender_docs td.obsolete0 a")
                            logging.info("  –ù–∞–π–¥–µ–Ω–æ %d –≤–ª–æ–∂–µ–Ω–∏–π ", len((links)))
                        except TimeoutException:
                            links = []
                            logging.warning("  –í–ª–æ–∂–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –Ω–∏ –ø–æ answ-file, –Ω–∏ –ø–æ tender_docs")
                    

                    # if not memory_manager.check_memory():
                    #     logging.warning("Memory usage critical, skipping file.")
                    #     break
                    links_info: list[tuple[str, str]] = []
                    for link in links:
                        href = link.get_attribute("href")
                        url = href if href.startswith("http") else f"{root}/{href.lstrip('/')}"
                        links_info.append((url, link.text.strip()))
                    # 2) –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
                    hits_found = False
                    max_threads = max(1, getattr(settings, "max_download_threads", 2))
                    with ThreadPoolExecutor(max_workers=max_threads) as executor:
                        futures = {}
                        for url, display_name in links_info:
                            if memory_manager.check_memory():
                                logging.warning("Memory usage critical, skipping file %s.", display_name)
                                continue
                            time.sleep(random.uniform(0.2, 0.5))
                            
                            fut = executor.submit(
                                _download_and_check,
                                url,
                                display_name,
                                session,
                                DOWNLOADS_DIR,
                                settings,
                                memory_manager,
                            )
                            futures[fut] = url
                        for fut in as_completed(futures):
                            try:
                                if fut.result():          # ‚Üê —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —Ñ–∞–π–ª ¬´—Å—Ä–∞–±–æ—Ç–∞–ª¬ª
                                    hits.append(tender_id)
                                    hits_found = True
                                    break
                            except Exception as e:
                                logging.error("–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ %s: %s", futures[fut], e)

                        if hits_found:
                            # –æ—Ç–º–µ–Ω—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –∑–∞–¥–∞—á–∏ (Python 3.11+)
                            executor.shutdown(cancel_futures=True)                        

                    # –æ—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–∏–º —Ç–µ–Ω–¥–µ—Ä–æ–º
                    shutil.rmtree(DOWNLOADS_DIR)
                    DOWNLOADS_DIR.mkdir(exist_ok=True)
                    if memory_manager:
                            logging.info("üì¶ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ –ø–µ—Ä–µ–¥ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º —Ç–µ–∫—Å—Ç–∞")
                            memory_manager.force_cleanup()

                    # –Ω–∞–∑–∞–¥ –∫ —Å–ø–∏—Å–∫—É
                    wait_click(driver, (By.ID, "back_button_2"))
                    WebDriverWait(driver, 30).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, "#list_apps_by_subject tbody tr"))
                    )

                # --- –ø–µ—Ä–µ—Ö–æ–¥ –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É --------------------------------
                if max_pages and page >= max_pages:
                    break
                try:
                    _next_page(driver)
                    page += 1
                except (TimeoutException, StopIteration):
                    break
        finally:
            if 'driver' in locals():
                driver.quit()

    with cache_path.open("w", encoding="utf-8") as f:
        for tid in sorted(visited):
            f.write(tid + "\n")
    return hits


if __name__ == "__main__":
    print(scrape_tenders(max_pages=1, headless=False))
